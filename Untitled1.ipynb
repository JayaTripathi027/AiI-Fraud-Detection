{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxguskLaB7lA"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, render_template, request, jsonify\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Define dataset path\n",
        "DATASET_PATH = r'/content/onlinefraud.csv'  # Ensure this file exists\n",
        "\n",
        "# Load dataset safely\n",
        "df = None  # Initialize as None\n",
        "\n",
        "if os.path.exists(DATASET_PATH):  # Check if file exists\n",
        "    try:\n",
        "        df = pd.read_csv(DATASET_PATH)\n",
        "        print(\"‚úÖ Dataset loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Error loading dataset:\", str(e))\n",
        "else:\n",
        "    print(f\"‚ùå ERROR: Dataset file not found at {DATASET_PATH}\")\n",
        "\n",
        "# Load trained models\n",
        "try:\n",
        "    xgb_model = pickle.load(open('/content/xgb.sav', 'rb'))\n",
        "    lr_model = pickle.load(open('/content/lr.sav', 'rb'))\n",
        "\n",
        "    print(\"‚úÖ Models loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error loading models:\", str(e))\n",
        "\n",
        "# Define important features\n",
        "important_features = ['type', 'amount', 'oldbalanceOrg', 'newbalanceOrig',\n",
        "                      'oldbalanceDest', 'newbalanceDest', 'isFlaggedFraud',\n",
        "                      'nameOrig', 'nameDest']\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_input(data):\n",
        "    try:\n",
        "        categorical_features = ['type', 'nameOrig', 'nameDest']  # Encode these\n",
        "        numeric_features = ['amount', 'oldbalanceOrg', 'newbalanceOrig',\n",
        "                            'oldbalanceDest', 'newbalanceDest', 'isFlaggedFraud']\n",
        "\n",
        "        # Transform categorical and numeric features\n",
        "        preprocessor = ColumnTransformer([\n",
        "            ('num', StandardScaler(), numeric_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "        ])\n",
        "\n",
        "        pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "        processed_data = pipeline.fit_transform(pd.DataFrame([data], columns=important_features))\n",
        "        return processed_data\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "# Home route\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template('index.html', features=important_features)\n",
        "\n",
        "# Prediction route\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    try:\n",
        "        # Collect user input\n",
        "        input_data = {feature: request.form[feature] for feature in important_features}\n",
        "\n",
        "        # Convert numeric values\n",
        "        numeric_keys = ['amount', 'oldbalanceOrg', 'newbalanceOrig',\n",
        "                        'oldbalanceDest', 'newbalanceDest', 'isFlaggedFraud']\n",
        "        for key in numeric_keys:\n",
        "            input_data[key] = float(input_data[key])\n",
        "\n",
        "        # Debugging logs\n",
        "        print(\"üü¢ Input Data:\", input_data)\n",
        "\n",
        "        # Preprocess input\n",
        "        final_features = preprocess_input(input_data)\n",
        "        print(\"üü¢ Processed Features Shape:\", final_features.shape)\n",
        "\n",
        "        # Make predictions\n",
        "        fraud_prediction = xgb_model.predict(final_features)[0]\n",
        "        fraud_prob = xgb_model.predict_proba(final_features)[0][1]\n",
        "\n",
        "        return render_template('index.html',\n",
        "                               prediction_text=f'Fraud Prediction: {\"Fraud\" if fraud_prediction else \"Not Fraud\"}',\n",
        "                               probability_text=f'Fraud Probability: {fraud_prob:.2f}',\n",
        "                               features=important_features)\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "# Run Flask app\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "id": "hwDLtHjMDcwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZZynbIo_Fj3q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}